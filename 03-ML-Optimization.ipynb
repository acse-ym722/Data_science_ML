{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a019dddd",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:maroon\">Machine Learning Optimization</h1>\n",
    "<img src=\"figures/03-Optimization.jpeg\" style=\"width:1300px;\">\n",
    "    <h3><span style=\"color: #045F5F\">Data Science & Machine Learning for Planet Earth Lecture Series</span></h3><h6><i> by C√©dric M. John <span style=\"size:6pts\">(2022)</span></i></h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdb092",
   "metadata": {},
   "source": [
    "## Plan for today's Lecture üóì "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb7116",
   "metadata": {},
   "source": [
    "* Understanding what fitting a model means\n",
    "* Brief introduction to Gradient Descent and other solvers\n",
    "* Regression loss Functions\n",
    "* Logistic function and cross entropy cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b381751",
   "metadata": {},
   "source": [
    "## Intended learning outcomes üë©‚Äçüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba65a35",
   "metadata": {},
   "source": [
    "* Understand weights and biases\n",
    "* Build an intuition for ML solvers\n",
    "* Choose an appropriate loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b48218",
   "metadata": {},
   "source": [
    "# Fitting parametric models\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_params.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>A 35 mm photo of the internal mechanism of a swiss mechanical clock, dramatic lighting</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60672ec4",
   "metadata": {},
   "source": [
    "### Example of Ordinary Least Square (OLS)\n",
    "A Linear Regression (OLS) maps a linear relationship between the input $X$ and the output $y$. It optimizes slope $a$ and intercept $b$ by reducing the residuals between the actual $y$ and the prediction $\\hat{y}$.\n",
    "$$\\hat{y} = aX+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47de8e",
   "metadata": {},
   "source": [
    "$$\\hat{y} = \\color{blue}{\\beta_0}+\\color{blue}{\\beta_1}X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84286059",
   "metadata": {},
   "source": [
    "In machine learning terminology, <span style=\"color:blue\">$\\beta_0$</span> (the intercept) is also known as a <span style=\"color:teal\">**bias**</span> (adds a constant) whereas <span style=\"color:blue\">$\\beta_{1}$</span> is referred to as a <span style=\"color:teal\">**weight**</span> (multiplies a feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee9617",
   "metadata": {},
   "source": [
    "## Today's example: Kaggle Fish Market dataset\n",
    "Chosen because it is simple: <a href=\"https://www.kaggle.com/aungpyaeap/fish-market\">Kaggle fish market dataset</a>.\n",
    "<img src=\"figures/fish_dataset.png\" style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c7096",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccd0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import IntSlider, interact, Button\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = pd.read_csv('data/fish.csv')\n",
    "data.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee065b",
   "metadata": {},
   "source": [
    "## Width vs Height for 'Breams' and 'Parkkis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e18624",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12, 6))\n",
    "data=data[(data.Species!='Bream') & (data.Species!='Parkki')]\n",
    "ax.scatter(data['Width'], data['Height'])\n",
    "ax.set_ylabel('Fish Height')\n",
    "ax.set_xlabel('Fish Width');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75b892",
   "metadata": {},
   "source": [
    "## 1. Fitting an Sklearn <code>LinearModel()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = MinMaxScaler().fit_transform(data[['Width']])\n",
    "y = data['Height']\n",
    "\n",
    "model = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473ff63",
   "metadata": {},
   "source": [
    "## 2. Plotting data and $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ce86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12, 6))\n",
    "ax.scatter(X, y)\n",
    "ax.plot(X, model.predict(X), c='r')\n",
    "ax.set_ylabel('Fish Height')\n",
    "ax.set_xlabel('Normalized Fish Width')\n",
    "ax.set_xlim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268933d",
   "metadata": {},
   "source": [
    "\n",
    "### Formulation\n",
    "Given a model expressed as $\\hat{y} = h(X, \\color{blue}{\\beta}) + error$<br> where:\n",
    "* $h(X,\\color{blue}{\\beta}) = \\color{blue}{\\beta_0} + \\color{blue}{\\beta_1} X_1$ in our example\n",
    "* $h$ is the **hypothesis** function\n",
    "* $h(X,\\color{blue}{\\beta})$ is the **prediction** ($\\hat{y})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2dde4a",
   "metadata": {},
   "source": [
    "Sklearn <code>.fit()</code> finds the best parameters to reduce the $error(X,y,\\color{blue}{\\beta})$<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011cd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_intercept = model.intercept_ # Beta0\n",
    "model_intercept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d92e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slope = model.coef_[0] # Beta1\n",
    "model_slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a162ac",
   "metadata": {},
   "source": [
    "\n",
    "**The Loss Function L**<br>\n",
    "In other words, <code>.fit()</code> minimizes the error on what is known as the loss function (<code>L(error)</code>).<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31973293",
   "metadata": {},
   "source": [
    "\n",
    "This can also be written as:<br> \n",
    "$$\\color{blue}{\\beta} = \\text{arg}\\min\\limits_{\\color{blue}{\\beta}}\\ L(\\color{blue}{\\beta}, X, y, h)$$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f903487",
   "metadata": {},
   "source": [
    "\n",
    "For ordinary least square regression $L$ is the squared error:<br>\n",
    "$$L_{OLS} = \\|error \\|^2 = \\|y -  \\color{blue}{\\color{blue}{\\beta}_0} - \\color{blue}{\\color{blue}{\\beta}_1} X_1\\|^2$$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb6dea",
   "metadata": {},
   "source": [
    "### Loss/Cost Functions $L$ vs performance metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452fc361",
   "metadata": {},
   "source": [
    "* A performance metric measures how well a model performs <span style=\"color:red\">***after fitting***</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9de721",
   "metadata": {},
   "source": [
    "* A <span style=\"color:teal\">**Loss function $L$**</span> is used to <span style=\"color:blue\">**fit** the model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92292e4c",
   "metadata": {},
   "source": [
    "* Both measure an error response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8a356",
   "metadata": {},
   "source": [
    "* Sometime, <span style=\"color:blue\">loss and performance metrics may be the same</span> (e.g. MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fb4da",
   "metadata": {},
   "source": [
    "* But loss needs to be <span style=\"color:teal\">**(sub)differentiable** (ie. smooth)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54aaa3",
   "metadata": {},
   "source": [
    "# Machine Learning Solvers\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_solvers.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>Photo of a dusty toolshed with metal tools hanging on a wall, washed out light</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3e7cdd",
   "metadata": {},
   "source": [
    "# Why we use solvers\n",
    "<img src=\"figures/OLS-vs-SGD.jpeg\" style=\"width:900px\"/>\n",
    "<a href=\"https://link.springer.com/chapter/10.1007/978-3-319-23036-8_25\">Menon & Namitha, 2015</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae252e",
   "metadata": {},
   "source": [
    "* Direct solution is <span style=\"color:red\">**not always possible:**</span> matrix inversion requires full rank matrices (same amount of columns than rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4e3b4",
   "metadata": {},
   "source": [
    "* **For large matrices**, matrix inversion is **<span style=\"color:red\">very computationally inefficient</span>** (see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e75bcce",
   "metadata": {},
   "source": [
    "* <span style=\"color:blue\">**SOLUTION:**</span> Approximate the solution through optimization (finding the minimum of a function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b88f27",
   "metadata": {},
   "source": [
    "# Intuition behind Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6666b75d",
   "metadata": {},
   "source": [
    "* Uses **the slope (gradient) of the Loss Function** as an indicator<br>\n",
    "* As the slope approaches zero, the Loss approaches its minimum<br>\n",
    "* Illustrated here for one of the two parameters of OLS (the intercept or the slope)<br>\n",
    "\n",
    "<img src=\"figures/gradient_descent.jpeg\" style=\"width:900px;\">\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/quick-guide-to-gradient-descent-and-its-variants-97a7afb33add\">Kansal, 2020</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a132b73",
   "metadata": {},
   "source": [
    "## 1-D gradient descent (finding the intercept $\\beta_{0}$)\n",
    "* Randomly initialize parameter value $\\color{blue}{\\beta_0}^\\color{red}{(0)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915437c8",
   "metadata": {},
   "source": [
    "* Compute the derivative of the <span style=\"color:blue\">loss function</span> ($\\frac{\\partial L}{\\partial \\color{blue}{\\beta_0}}$) at point $\\beta_{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c167985",
   "metadata": {},
   "source": [
    "* Update the $\\beta_{0}$ proportionally to $\\frac{\\partial L}{\\partial \\color{blue}{\\beta_0}}$ and to a given step size known as the <span style=\"color:teal\">***learning rate*** ($\\eta$)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91422b1",
   "metadata": {},
   "source": [
    "* $\\color{blue}{\\beta_0}^{\\color {red}{(k+1)}} = \\color{blue}{\\beta_0}^{\\color {red}{(k)}} - {\\eta} \\frac{\\partial L}{\\partial \\color{blue}{\\beta_0}}(\\color{blue}{\\beta_0}^{\\color{red}{(k)}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ea8d9",
   "metadata": {},
   "source": [
    "* One update cycle where all of the training data is seen is known as an <span style=\"color:teal\">**epoch**</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4396fcee",
   "metadata": {},
   "source": [
    "* Repeat as many **epochs** as necessary for our algorithm to converge to our stopping criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152f82d",
   "metadata": {},
   "source": [
    "## Loss Function for our regression (Fish Market Dataset)\n",
    "$$MSELoss = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\beta_{0}-\\beta_{1}*X_{i})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92397a56",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial\\ MSELoss}{\\partial\\ \\beta_{0}} =  \\frac{1}{n}\\sum_{i=1}^{n}-2*(y_{i}-\\beta_{0}-\\beta_{1}*X_{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bea6c7",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial\\ MSELoss}{\\partial\\ \\beta_{0}} =  -\\frac{2}{n}\\sum_{i=1}^{n}(y_{i}-\\beta_{1}*X_{i}-\\beta_{0})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941c7eb",
   "metadata": {},
   "source": [
    "# Gradient Descent $\\eta$ =0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cde79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solvers import GradientDescent\n",
    "epochs = 1500;eta=.7;initial_beta0=-.01;\n",
    "gd = GradientDescent(initial_beta0, eta, epochs, model_slope, X, y, model_intercept, model_slope).regression_vs_gradient();\n",
    "middle_case = [gd.betazeros, gd.losses, gd.gradients]\n",
    "interact(update_epoch, epoch=IntSlider(min=0,max=epochs-1,value=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25eb04d",
   "metadata": {},
   "source": [
    "# Stopping Criterions\n",
    "\n",
    "Gradient Descent can have different **stopping criterions** :<br>\n",
    "\n",
    "* **Minimum Step Size** (e.g. 0.001). When the step size is smaller, the Gradient Descent has converged, and the corresponding intercept is the optimal value.\n",
    "* **Maximum number of steps** (e.g. 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c664c0",
   "metadata": {},
   "source": [
    "# Gradient Descent $\\eta$ =0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1500;eta=.05;initial_beta0=-.01;\n",
    "gd = GradientDescent(initial_beta0, eta, epochs, model_slope, X, y, model_intercept, model_slope).regression_vs_gradient();\n",
    "low_case = [gd.betazeros, gd.losses, gd.gradients]\n",
    "interact(update_epoch, epoch=IntSlider(min=0,max=epochs-1,value=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db91c2",
   "metadata": {},
   "source": [
    "# Gradient Descent $\\eta$ =1.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cf780",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1500;eta=1.04;initial_beta0=-.01;\n",
    "gd = GradientDescent(initial_beta0, eta, epochs, model_slope, X, y, model_intercept, model_slope).regression_vs_gradient();\n",
    "high_case = [gd.betazeros, gd.losses, gd.gradients]\n",
    "interact(update_epoch, epoch=IntSlider(min=0,max=epochs-1,value=0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize=(20,6))\n",
    "      \n",
    "for ax, index, title in zip(axes.flatten(),[0,1,2],['Intercept', 'Loss', 'Gradient']):\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    \n",
    "    for data, label in zip([low_case, middle_case], [r'$\\eta=0.05$',r'$\\eta=0.7$']):\n",
    "        ax.plot(range(len(data[0])),data[index], label=label)\n",
    "        ax.set_xlim(0,70) \n",
    "        ax.legend()\n",
    "        \n",
    "axes[0].set_ylim(0,3); axes[1].set_ylim(0.3,3); axes[2].set_ylim(2,-3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eef37a",
   "metadata": {},
   "source": [
    "## Effect of Learning Rate\n",
    "**Small learning rate**<br>\n",
    "\n",
    "* Smoother path to mimimum\n",
    "* Requires more epochs\n",
    "* May get stuck at local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f73ef2",
   "metadata": {},
   "source": [
    "**Large learning rate**<br>\n",
    "\n",
    "* Requires less epochs\n",
    "* More random at first\n",
    "* May never converge!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e7f27",
   "metadata": {},
   "source": [
    "üí° **Descent always converges faster when features are scaled**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc137a86",
   "metadata": {},
   "source": [
    "## 2D descent : How to co-optimize $\\color{blue}{\\beta_0}$ and $\\color{blue}{\\beta_1}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd04403",
   "metadata": {},
   "source": [
    "The Loss function would be 3-dimensional and look something like this.\n",
    "\n",
    "<img src=\"figures/2D_energy_landscape.png\" style=\"width:1200px\">\n",
    "<a href=\"http://primo.ai/index.php?title=Gradient_Descent_Optimization_%26_Challenges\">From Primo.ai</a>\n",
    "\n",
    "* This is called the **Energy Landscape** of the loss function<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c04ca",
   "metadata": {},
   "source": [
    "* Use the **same principle** as above: calculate partial derivative for each term at all steps<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ad017",
   "metadata": {},
   "source": [
    "* Generalizes to **$n$ parameters** (dimensions)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61e7f29",
   "metadata": {},
   "source": [
    "## Vector formulation of gradient descent for $n$ parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9762b6e4",
   "metadata": {},
   "source": [
    "Start with random values of $\\color{blue}{\\beta}$ (epoch 0)<br>\n",
    "\n",
    "At each **epoch** update all parameters $(\\color{blue}{\\beta_0}^{\\color {red}{(k+1)}}, \\color{blue}{\\beta_1}^{\\color {red}{(k+1)}},..., \\color{blue}{\\beta_n}^{\\color {red}{(k+1)}}$) in the direction of \"downard pointing gradient\" with a learning rate $\\eta$ (eta)<br>\n",
    "\n",
    "\n",
    "$\\color{blue}{\\beta}_0^{\\color {red}{(k+1)}} = \\color{blue}{\\beta_0}^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\color{blue}{\\beta_0}}(\\color{blue}{\\beta}^{\\color{red}{(k)}})$ <br>\n",
    "$\\color{blue}{\\beta_1}^{\\color {red}{(k+1)}} = \\color{blue}{\\beta_1}^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\color{blue}{\\beta_1}}(\\color{blue}{\\beta}^{\\color {red}{(k)}})$<br>\n",
    "$                      [...] $<br>\n",
    "$\\color{blue}{\\beta_n}^{\\color {red}{(k+1)}} = \\color{blue}{\\beta_n}^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\color{blue}{\\beta_n}}(\\color{blue}{\\beta}^{\\color {red}{(k)}})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3d07e",
   "metadata": {},
   "source": [
    "\n",
    "We can consider the vector of partial derivatives, called the **gradient** vector (**nabla**:$\\nabla$)$${\\displaystyle \\nabla L(\\color{blue}{\\beta})={\\begin{bmatrix}{\\frac {\\partial L}{\\partial \\color{blue}{\\beta_{0}}}}(\\color{blue}{\\beta})\\\\\\vdots \\\\{\\frac {\\partial L}{\\partial \\color{blue}{\\beta_{n}}}}(\\color{blue}{\\beta})\\end{bmatrix}}.}$$üí° hence the name <em>Gradient Descent</em><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9946bd",
   "metadata": {},
   "source": [
    "\n",
    "Gradient descent - vector formula:\n",
    "$$\\color{blue}{\\beta}^{\\color {red}{(k+1)}} = \\color{blue}{\\beta}^{\\color {red}{(k)}} - \\eta \\ \\nabla L(\\color{blue}{\\beta}^{\\color{red}{(k)}})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ed682",
   "metadata": {},
   "source": [
    "# Modfied versions of Gradient Descent\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_sgd.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>A futuristic looking formula 1 car is driving very fast down a busy market street</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a715d9",
   "metadata": {},
   "source": [
    "\n",
    "üëé Gradient Descent is **computationally expensive** on big datasets<br>\n",
    "At **each epoch** we evaluate $\\nabla L$  using all $n$ observations, for each $x$ features. Other approaches exist.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e45ad9",
   "metadata": {},
   "source": [
    "\n",
    "### Classical Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097d2f2",
   "metadata": {},
   "source": [
    "\n",
    "* Loop one by one over all $n$ observations\n",
    "* Select a **single, randomly selected data point** \n",
    "* Compute the Loss/gradient for this single point\n",
    "* update all the <span style=\"color:blue\">$\\beta$</span>\n",
    "* Once all $n$ observations have been viewed, repeat another epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0912646",
   "metadata": {},
   "source": [
    "\n",
    "### Batch or mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f67970",
   "metadata": {},
   "source": [
    "At each iteration, compute an **\"approximate\" Loss** and take one step against its gradient<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61002242",
   "metadata": {},
   "source": [
    "Choose minibatch size (e.g. 16)<br>\n",
    "\n",
    "Loop over your $n$ observations, minibatch-per-minibatch<br>\n",
    "\n",
    "For each minibatch calculate $\\nabla L_{mini}$\n",
    "Use this gradient to update $\\beta^{\\color {red}{(k+1)}} = \\beta^{\\color {red}{(k)}} - \\eta \\ \\nabla L_{mini}(\\beta^{\\color{red}{(k)}})$\n",
    "Move to next $>X_{mini}$ (e.g. the 16-32 observations)\n",
    "\n",
    "\n",
    "Once all $n$ observations have been viewed, repeat another **epoch**<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58fcf4b",
   "metadata": {},
   "source": [
    "By working on a single point rather than the dataset average, the SGD is less stable.<br>\n",
    "\n",
    "The **Loss fluctuates** from epoch to epoch more and does not necessarily decrease.<br>\n",
    "As a result, the **steps** taken are **less direct** towards the minimum\n",
    "\n",
    "<img src=\"figures/SGD.png\" style=\"width:1300px\"/>\n",
    "<img src=\"figures/loss_bgd_sgd.png\" style=\"width:1300px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193e73b3",
   "metadata": {},
   "source": [
    "<font color=\"green\">Pros</font><br>\n",
    "\n",
    "SGD is faster for very large datasets\n",
    "Jumps out of local minima!\n",
    "Allows to reduce RAM load (see deep learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd678e",
   "metadata": {},
   "source": [
    "<font color=\"red\">Cons</font><br>\n",
    "\n",
    "Need more epochs\n",
    "Never exactly converges (careful when to stop?)\n",
    "Maybe slower for small $p$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0c05e",
   "metadata": {},
   "source": [
    "# SGD Algorithms in Scikit-Learn\n",
    "\n",
    "* For large dataset, it is much faster to use **SGD** in Scikit-Learn\n",
    "* Two classes: <code>SGDRergessor</code> and <code>SGDClassifier</code>\n",
    "* The choice of loss function determines the exact algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd = SGDRegressor(loss='squared_error', eta0=0.5)\n",
    "\n",
    "sgd.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12, 6))\n",
    "ax.scatter(X, y)\n",
    "ax.plot(X, model.predict(X), c='r', label='LinearRegressor()')\n",
    "ax.plot(X, sgd.predict(X), c='purple', label='SGDRegressor(loss=\"squared_error\")')\n",
    "ax.set_ylabel('Fish Height')\n",
    "ax.set_xlabel('Normalized Fish Width')\n",
    "ax.set_xlim(0,1); ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e6b2c",
   "metadata": {},
   "source": [
    "# Second Order Derivative (Hessian) Solvers [in brief]\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_hessian.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>Sunrise on the town of Hesse in Germany by a hazzy autumn day, golden colors, filtered light</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bce37f",
   "metadata": {},
   "source": [
    "### General Idea:\n",
    "By incorporating informations from the <span style=\"color:teal\"> **second derivative (Hessian)**</span> of the function, we should be able to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7d66a",
   "metadata": {},
   "source": [
    "1. **Converge faster** to the global minima of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab1cf9",
   "metadata": {},
   "source": [
    "2. Avoid **local minima**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc3231",
   "metadata": {},
   "source": [
    "### Newton's Method\n",
    "* Newton is the most basic second order solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d77c82",
   "metadata": {},
   "source": [
    "* Centered around a quadratic approximation of $f$ for points near $xn$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5effb0",
   "metadata": {},
   "source": [
    "$$f(x+\\Delta x)‚âàf(x)+\\Delta x^{T}\\nabla f(x)+\\frac{1}{2}\\Delta x^{T}(\\nabla^{2}f(x))\\Delta x$$\n",
    "Where $\\nabla$ are the gradient and $\\nabla^{2}$ the Hessian of the function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3203781",
   "metadata": {},
   "source": [
    "#### Newton's method bottom line:\n",
    "* will converge to a unique global minimizer for convex function. \n",
    "* For non-convex functions will still works but is only guranteed to converge to a local minimum. \n",
    "* Calculating the Hessian for large parameter space is impractical (too computationally expansive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70904ec6",
   "metadata": {},
   "source": [
    "### Limited-memory Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno algorithm (L-BFGS)\n",
    "* L-BFGS is a ***Quasi-Newton*** method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b201238",
   "metadata": {},
   "source": [
    "* The idea is to <span style=\"color:red\">**approximate**</span> the Hessian rather than exactly calculate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905fd84",
   "metadata": {},
   "source": [
    "* Much faster to compute than Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daad179",
   "metadata": {},
   "source": [
    "* L-BFGS has been called <span style=\"color:teal\">**\"the algorithm of choice\"**</span> for fitting log-linear (MaxEnt) models and conditional random fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeff0eb",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_loss.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>A british tourist lost in the street of Beijing amongst a crowd of busy people, dramatic lighting</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d947f2",
   "metadata": {},
   "source": [
    "## Regression Loss Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76f869",
   "metadata": {},
   "source": [
    "### L1 (MAE) vs L2 (MSE) loss\n",
    "\n",
    "$$ L_1 = MAE = \\frac{1}{n}\\sum_{i=0}^n | \\hat{y}_i - y_i|\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:L_2 = MSE = \\frac{1}{n}\\sum_{i=0}^n (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "<img src=\"figures/mse_vs_mae_learning.png\" style=\"width:2000px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453687f",
   "metadata": {},
   "source": [
    "* MSE very sensitive to outliers vs. MAE less strict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc986e30",
   "metadata": {},
   "source": [
    "* MAE requires learning rate $\\eta$ which decreases at every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f41519",
   "metadata": {},
   "source": [
    "### Huber loss (mixed L1 and L2 losses, also called Smooth Absolute or Smooth L1)\n",
    "<img src=\"figures/smooth_L1.png\" style=\"width:900px\"/>\n",
    "<a href=\"https://link.springer.com/article/10.1007/s11263-019-01275-0\">Image from Feng et al 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81227e6",
   "metadata": {},
   "source": [
    "* Parametrized MAE which becomes MSE when error is small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d3d1f",
   "metadata": {},
   "source": [
    "$L_{\\delta} = \\left\\{\n",
    "  \\begin{array}{lr}\n",
    "    \\frac{1}{2}(y - \\hat{y})^{2} & : |y - \\hat{y}| \\leq \\delta\\\\\n",
    "    \\delta*(|y-\\hat{y}|-\\frac{1}{2}*\\delta) & :  |y - \\hat{y}| > \\delta\n",
    "  \\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f15c92",
   "metadata": {},
   "source": [
    "* Adjustable for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f765a",
   "metadata": {},
   "source": [
    "* Slope can be used as an indicator of reaching minima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5bca09",
   "metadata": {},
   "source": [
    "## Classification Losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470da0bd",
   "metadata": {},
   "source": [
    "### Sarting Point: Logistic classifiers\n",
    "<img src=\"figures/log_lin.png\" style=\"width:1500px\"/><br>\n",
    "<a href=\"https://medium.datadriveninvestor.com/logistic-regression-18afd48779ce\">Maheshwari, V, 2018</a>\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10428a5",
   "metadata": {},
   "source": [
    "### We can apply $\\sigma$ to our **hypothesis function**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9379a3",
   "metadata": {},
   "source": [
    "$h = \\color{blue}{\\beta}_0 + \\sum_{i=1}^{n}\\color{blue}{\\beta}_i x_i = \\color{blue}{\\beta}^{T}x$ (vectorized form of the hypothesis function where $x_0$ is always set to be 1)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14375b5",
   "metadata": {},
   "source": [
    "The logistic regression assumes that for a single point $x$ the probability of $y=1$ is the logistic function applied to $h$:<br>\n",
    "$P(Y=1 | X=x) = \\sigma (\\color{blue}{\\beta}^{T}x)$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4c877",
   "metadata": {},
   "source": [
    "This is also often written as:<br>\n",
    "$P(Y=1 | X=x) = \\sigma (\\color{blue}{\\beta}^{T}x)$ <br>\n",
    "$P(Y=0 | X=x) = 1 - \\sigma (\\color{blue}{\\beta}^{T}x)$ by law of probability<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc6cefc",
   "metadata": {},
   "source": [
    "### Likelihood function\n",
    "In binary classification the outcomes follow a Bernoulli distribution and we can write our pobability function as follows:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0c694",
   "metadata": {},
   "source": [
    "$P(Y=y | X=x) = \\sigma (\\color{blue}{\\beta}^{T}x)^{y}[1-\\sigma (\\color{blue}{\\beta}^{T}x)]^{1-y}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c011ff7",
   "metadata": {},
   "source": [
    "$P(Y=y | X=x) = \\left\\{\n",
    "  \\begin{array}{lr}\n",
    "    \\sigma (\\color{blue}{\\beta}^{T}x)\\leftarrow y=0\\\\\n",
    "    [1-\\sigma (\\color{blue}{\\beta}^{T}x)]\\leftarrow y=1\n",
    "  \\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5109c3",
   "metadata": {},
   "source": [
    "### Log Likelihood\n",
    "It is easier to work with log of the likelihood function:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853cab8",
   "metadata": {},
   "source": [
    "$\\log{P(Y=y | X=x)} = \\log{[\\sigma (\\color{blue}{\\beta}^{T}x)^{y}[1-\\sigma (\\color{blue}{\\beta}^{T}x)]^{1-y}]}=\\log{[\\sigma(\\color{blue}{\\beta}^{T}x)]}+(1-y)\\log{[1-\\sigma(\\color{blue}{\\beta}^{T}x)]} $<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6784cd",
   "metadata": {},
   "source": [
    "The **log loss** or **binary cross-entropy loss** is simply the negative of the log likelihood so that we can **<i>maximize</i>** it with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b37a2",
   "metadata": {},
   "source": [
    "$$LL_{CE}(y,\\hat{y})= -\\log{[\\sigma(\\color{blue}{\\beta}^{T}x)]}+(1-y)\\log{[1-\\sigma(\\color{blue}{\\beta}^{T}x)]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c19be",
   "metadata": {},
   "source": [
    "The **<i>loss</i>** function is calculated for 1 sample, the **<i>cost</i>** function for all samples is the **average of all losses**:<br>\n",
    "$$LL_{CE}(\\color{blue}{\\beta}) = - \\frac{1}{n} \\sum_{i=1}^{n} y_i\\log\\sigma(\\color{blue}{\\beta}^{T}x_i)+(1-y_i)\\log[1-\\sigma(\\color{blue}{\\beta}^{T}x_i)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5694ed21",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss\n",
    "<img src=\"figures/logloss.png\" style=\"width:2000px\"/>\n",
    "‚òùÔ∏è Infinitely penalize wrong prediction<br/>\n",
    "üí° Cross-Entropy name comes from <a href=\"https://www.youtube.com/watch?v=ErfnhcEV1O8\">Shanon Theory of information</a><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a85afe",
   "metadata": {},
   "source": [
    "\n",
    "ü§î The gradient of the **log-loss** of the **sigmoid** function is simple in vectorial form<br>\n",
    "$\\nabla LogLoss_{sigmoid} = - \\frac{2}{n} \\beta^T(y - \\hat{y})$<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe9257",
   "metadata": {},
   "source": [
    "\n",
    "Exact **same formula** than that of the mse-loss of a linear regression<br>\n",
    "$\\nabla MSE_{linear} = - \\frac{2}{n} \\beta^T(y - \\hat{y})$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7de9d2",
   "metadata": {},
   "source": [
    "\n",
    "<br/><br>\n",
    "‚ö†Ô∏è These gradients **do not have the same value** of course as:<br>\n",
    "* $\\hat{y}_{sigmoid} = \\frac{1}{1+e^{-\\color{blue}{\\beta}^T}}$\n",
    "* $\\hat{y}_{linear} = \\color{blue}{\\beta}^{T}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d3a70d",
   "metadata": {},
   "source": [
    "# Suggested Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56746b",
   "metadata": {},
   "source": [
    "## üì∫ Videos \n",
    "#### Short videos from my Undegraduate Machine Learning Classes:\n",
    "* üìº <a href=\"https://youtu.be/l6mTzbPOWHE?list=PLZzjCZ3QdgQCcRIwQdd-_cJNAUgiEBB_n\">Gradient Descent</a>\n",
    "\n",
    "#### Others:\n",
    "\n",
    "* üìº <a href=\"https://www.youtube.com/watch?v=sDv4f4s2SB8\">StatsQuest - Gradient Descent</a> by Josh Starmer\n",
    "* üìº <a href=\"https://youtu.be/vMh0zPT0tLI\">StatsQuest - Stockastic Gradient Descent Clearly Explained</a> by Josh Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1c29a",
   "metadata": {},
   "source": [
    "## üìö Further Reading \n",
    "* üìñ <a href=\"https://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/\">Hessian Free Optimization</a> by Andrew Gibiansky\n",
    "* üìñ <a href=\"https://ruder.io/optimizing-gradient-descent/\">An overview of gradient descent optimization algorithms</a> by Sebastien Ruder, 2016"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
