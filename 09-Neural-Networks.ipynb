{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b136c1ec",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:maroon\">An Introduction to Neural Networks</h1>\n",
    "    <img src=\"figures/09-neural_networks.jpeg\" style=\"width:1300px\">\n",
    "    <h3><span style=\"color: #045F5F\">Data Science & Machine Learning for Planet Earth Lecture Series</span></h3><h6><i> by C√©dric M. John <span style=\"size:6pts\">(2022)</span></i></h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abbaa4d",
   "metadata": {},
   "source": [
    "## Plan for today's Lecture üóì "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c968e",
   "metadata": {},
   "source": [
    "* Introduction to <code>TensorFlow.keras</code>\n",
    "* Overview of the building blocks of neural networks\n",
    "* Compiling and training neural networks\n",
    "* Deep-Learning: adapting neural network architectures to specific tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec80f0",
   "metadata": {},
   "source": [
    "## Intended learning outcomes üë©‚Äçüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4777fe8e",
   "metadata": {},
   "source": [
    "* Be comfortable using <code>TensorFlow.keras</code>\n",
    "* Correctly selet the type of neural layer for your task\n",
    "* Compile, train and assess neural networks\n",
    "* Gain confidence in the method in preparation for your Deep-Learning module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876419b1",
   "metadata": {},
   "source": [
    "http://beamlab.org/deeplearning/2017/02/23/deep_learning_101_part1.html\n",
    "\n",
    "https://erogol.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa0449",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_perceptron.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>A photo of the Mark I Perceptron machine in dramatic lighting with scientists in white lab coat fretting about</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e355c4",
   "metadata": {},
   "source": [
    "# Neural Network: A Historical Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f933830",
   "metadata": {},
   "source": [
    "#### Neural Networks have a long history starting with the \"Perceptron\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367c68b",
   "metadata": {},
   "source": [
    "<img src=\"figures/Mark_I_perceptron.jpeg\" style=\"height:600px;padding:5px\" align=\"left\">\n",
    "<img src=\"figures/Rosenblatt.jpg\" style=\"height:600px;padding:3px\" align=\"left\">\n",
    "<a href=\"https://en.wikipedia.org/wiki/Perceptron\">Wikipedia</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095bf50",
   "metadata": {},
   "source": [
    "## Neural Network date back to the 70's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d611d3",
   "metadata": {},
   "source": [
    "<img src=\"figures/ML-popularity.jpg\" style=\"width:1600px\">\n",
    "<a href=\"https://erogol.com/\">Eren G√∂lge, 2015</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f73ec",
   "metadata": {},
   "source": [
    "## Technical Advancements for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032424a",
   "metadata": {},
   "source": [
    "<img src=\"figures/nn_timeline.jpg\" style=\"width:1600px\">\n",
    "<a href=\"http://beamlab.org/deeplearning/2017/02/23/deep_learning_101_part1.html\">Beam, A., 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7a332",
   "metadata": {},
   "source": [
    "### Compute Power using Graphic Processor Units (GPUs) and the rise of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08196c",
   "metadata": {},
   "source": [
    "<img src=\"figures/imagenet_progress.png\" style=\"width:1300px\">\n",
    "<a href=\"http://beamlab.org/deeplearning/2017/02/23/deep_learning_101_part1.html\">Beam, A., 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b5e559",
   "metadata": {},
   "source": [
    "### Availability of (Labelled) Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e198c",
   "metadata": {},
   "source": [
    "<img src=\"figures/ai-winter.png\" style=\"width:1500px\">\n",
    "<a href=\"https://link.springer.com/article/10.1007/s10506-022-09309-8\">Francesconi, 2022</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e5d63",
   "metadata": {},
   "source": [
    "### Neural Networks (and the field of Deep-Learning) are a subfield of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25539225",
   "metadata": {},
   "source": [
    "<img src=\"figures/Circles.png\" style=\"width:600px\">\n",
    "<a href=\"https://link.springer.com/article/10.1007/s10506-022-09309-8\">Francesconi, 2022</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de1381",
   "metadata": {},
   "source": [
    "üìÉ **Deep Learning not (yet) better** than other ML approaches at predicting tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3540e",
   "metadata": {},
   "source": [
    "üñºÔ∏è **Deep Learning** very **powerful with unstructured data** (images, NLP, sound, video, ...) and some highly structured data (time series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0c8a5",
   "metadata": {},
   "source": [
    "üèéÔ∏è **Most recent progress in ML** are in the field of Deep-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2617c9b",
   "metadata": {},
   "source": [
    "### In the `Deep-Learning` module you will learn to handle complex model architecture\n",
    "\n",
    "\n",
    "<img src=\"figures/model_zoo1.png\" style=\"width:250px\" align=\"left\">\n",
    "<img src=\"figures/model_zoo2.jpeg\" style=\"height:700px\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd538f",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ These will include e.g. <span style=\"color:blue\">**Convolutional Neural Networks**</span>, <span style=\"color:brown\">**Transformers**</span> and <span style=\"color:purple\">**Recurrent Neural Networks**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df7042",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ Today, we are limiting ourselves to <span style=\"color:teal\">**Feedforward Neural Networks**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92170b2",
   "metadata": {},
   "source": [
    "# Ok, but what ARE Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f4cfd2",
   "metadata": {},
   "source": [
    "## Biological Neurons\n",
    "\n",
    "<img src=\"figures/Neuron3.png\" style=\"width:1000px\">\n",
    "<a href=\"https://www.youtube.com/watch?v=_HMLZHQpQDI\">Krigolson, 2019</a> (YouTube video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87ef69",
   "metadata": {},
   "source": [
    "* üî£ Biological neurons take inputs through their dendrite, transform the input, yield an output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31094f6",
   "metadata": {},
   "source": [
    "* üî• The neuron <span style=\"color:brown\">**only fires if a threshold in signal is reached**</span> (non-linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd123e6b",
   "metadata": {},
   "source": [
    "* ü§ñ Artificial neurons are <span style=\"color:blue\">*loosely*</span> inspired from the biological neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d848d030",
   "metadata": {},
   "source": [
    "## Artificial Neurons\n",
    "\n",
    "The combination of the linear regression followed by an activation function is effectively what is known as an **ARTIFICIAL NEURON**!\n",
    "\n",
    "<img src=\"figures/neuron.png\" style=\"width:800px\">\n",
    "<a href=\"https://www.amazon.com/dp/0131471392\">Haykin, 2008</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c8f5bb",
   "metadata": {},
   "source": [
    "* Notice the <span style=\"color:blue\">**Activation Function**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acbab37",
   "metadata": {},
   "source": [
    "### Let's write one from scratch in Python!\n",
    "\n",
    "Neural networks are surprisingly easy to code. Let's imagine that we have feature vector (`X`, shape=4) that pertain to weather, and a label (`y`) that indicates whether it will rain (`1`) or not (`0`) in the next hour. Here is how a sample would look:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe70334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example of a 'rainy day':\n",
    "\n",
    "y = 1 \n",
    "X = [1., -3.1, -7.2, 2.1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655da211",
   "metadata": {},
   "source": [
    "‚òî We want to **predict whether or not it will rain**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ff71b3",
   "metadata": {},
   "source": [
    "We can write a function that returns a linear regression wiht some weights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_1(X):\n",
    "    return -3 + 2.1*X[0] - 1.2*X[1] + 0.3*X[2] + 1.3*X[3]\n",
    "\n",
    "out_1 = linreg_1(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f404e",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "\n",
    "* üìè As writen above, our <span style=\"color:red\">algorithm is simply a linear regression</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b463eaa",
   "metadata": {},
   "source": [
    " \n",
    "* ‚ùÑÔ∏è The trick is to take the output of the linear function, and <span style=\"color:blue\">transform it via an activation function</code>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db9af3",
   "metadata": {},
   "source": [
    "* üöß The activation function will <span style=\"color:teal\">only output the value if certain conditions are met</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80191152",
   "metadata": {},
   "source": [
    "**Some well-known activation functions:**\n",
    "<img src=\"figures/activation_functions.png\" style=\"width:1000px\">\n",
    "                                                   \n",
    "<a href=\"https://medium.com/@shrutijadon/survey-on-activation-functions-for-deep-learning-9689331ba092\"> Jadon, 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b8a2f",
   "metadata": {},
   "source": [
    "<span style=\"color:brown\">(**ReLU** most commonly used these days)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09d960",
   "metadata": {},
   "source": [
    "### Implementing the 'ReLU' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a435533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    if x > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "out_1 = activation(out_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61883712",
   "metadata": {},
   "source": [
    "### Adding more neurons with the same inputs but different weights\n",
    "\n",
    "We can\n",
    "* apply **other** linear regressions (neurons) to the same input X\n",
    "* followed by the **same** activation function\n",
    "* but with different **(trainable) weights and biases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_2(X):\n",
    "    return -5 - 0.1*X[0] + 1.2*X[1] + 4.9*X[2] - 3.1*X[3]\n",
    "\n",
    "out_2 = activation(linreg_2(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a8d7e",
   "metadata": {},
   "source": [
    "and:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2007db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_3(X):\n",
    "    return -8 + 0.4*X[0] + 2.6*X[1] +- 2.5*X[2] + 3.8*X[3]\n",
    "\n",
    "out_3 = activation(linreg_3(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa80a2a",
   "metadata": {},
   "source": [
    "### We just wrote a layer of neurons!\n",
    "Each neuron receives the same input (`X`), has different weights, and uses the same activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c40b5",
   "metadata": {},
   "source": [
    "In neural networks, the next step is to give the output of these neurons as input to the next layer of neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06111ab",
   "metadata": {},
   "source": [
    "## Building a Neural Network\n",
    "\n",
    "A neural network is a complex function $f_{\\theta}$:\n",
    "$$f_{\\theta}(X) = \\hat{y}$$\n",
    "\n",
    "Where <span style=\"color:blue\">$X$</span> is the feature vectors, <span style=\"color:purple\">$\\theta$</span> are the weights and biases of the linear regressions that take place within each neuron, and <span style=\"color:brown\">$\\hat{y}$</span> is the prediction output of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553c27f",
   "metadata": {},
   "source": [
    "![net](figures/neuralnet_4.png)\n",
    "<a href=\"http://www.astroml.org/_images/fig_neural_network_1.png\">≈Ω. Iveziƒá et al., 2014</a>\n",
    "\n",
    "The way the neurons and the weight and biases are connected is known as the **architecture** of your neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a70a5",
   "metadata": {},
   "source": [
    "### Implementing the next layer (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46429504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linreg_next_layer(X):\n",
    "    return 5.1 + 1.1*X[0] - 4.1*X[1] - 0.7*X[2]\n",
    "\n",
    "def activation_next_layer(x):\n",
    "    # this is known as the sigmoid activation, used for clasification task!\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "def neural_net_predictor(X):\n",
    "    \n",
    "    out_1 = activation(linreg_1(X))\n",
    "    out_2 = activation(linreg_2(X))\n",
    "    out_3 = activation(linreg_3(X))\n",
    "    \n",
    "    outs = [out_1, out_2, out_3]\n",
    "    \n",
    "    y_pred = activation_next_layer(linreg_next_layer(outs))\n",
    "    \n",
    "    return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final prediction\n",
    "y_pred = neural_net_predictor(X)\n",
    "\n",
    "print(f' Probability of rain: {y_pred:.02f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a60a4",
   "metadata": {},
   "source": [
    "### üéâ Congrats! You just build your first (artificial) neural network\n",
    "\n",
    "* and because it is in pure `Python`, it is **super inefficent**!\n",
    "* Also, the weights and biases in our function are fixed (not trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4d124",
   "metadata": {},
   "source": [
    "## Deep-Learning with <code>scikit-learn</code>\n",
    "\n",
    "Although there is a module called <code>neural_network</code> in <code>scikit-learn</code>, it contains only three algorithms (<code>BernoulliRBM()</code>,<code>MLPClassifier()</code>, and <code>MLPRegressor()</code> and, in practice, it is very limited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b34bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,45, 100,23), activation='relu', max_iter=1000)\n",
    "\n",
    "x = [[1.2,0.1], [2.3,0.4], [1.3,0.2], [1.5,0.2], [4.6,0.12], [2.3,0.23]]\n",
    "y = [0,1,1,1,0,1]\n",
    "\n",
    "mlp.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d629ab",
   "metadata": {},
   "source": [
    "üö® This works, but we are not able to devise our own architecture beyond deciding how many layers and number of neuron per layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764e4cb",
   "metadata": {},
   "source": [
    "üí° For this, we need a framework specific for Deep-Learning: the two most popular today are <code>TensorFlow.keras</code> written and supported by **Google**, and <code>PyTorch</code> written and supported by **Meta**. Let's explore their popularity!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0d16d",
   "metadata": {},
   "source": [
    "# Introduction to <code>TensorFlow.keras</code>\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_tfkeras.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>The tensorflow keras popular deep-learning library having a drink with a few friends, digital art</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e35fca",
   "metadata": {},
   "source": [
    "### The PyTorch vs TensorFlow War: Google Searches\n",
    "<img src=\"figures/tf_vs_pt.png\" style=\"width:300px\">\n",
    "<img src=\"figures/tensorflow-vs-pytorch.png\" style=\"width:1800px\">\n",
    "<a href=\"https://buggyprogrammer.com/pytorch-vs-tensorflow-which-one-is-better/\">Kumar, A., 2022</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46a3d3",
   "metadata": {},
   "source": [
    "### PyTorch is now the preferred framework for research...\n",
    "<img src=\"figures/research_papers.png\" style=\"width:800\">\n",
    "<a href=\"https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/\">O'Connor, 2021</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde13d0a",
   "metadata": {},
   "source": [
    "### ... but TensorFlow remains the industry standard\n",
    "<img src=\"figures/tf_jobs.png\" style=\"width:800\">\n",
    "<a href=\"https://www.reddit.com/r/MachineLearning/comments/rga91a/d_are_you_using_pytorch_or_tensorflow_going_into/\">O'Connor, 2021</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39b617",
   "metadata": {},
   "source": [
    "## We believe that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe95b6",
   "metadata": {},
   "source": [
    "üöÄ Fundamental principles matter more than what framework you use!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2e8c4",
   "metadata": {},
   "source": [
    "üç∞ We will uee <code>TensorFlow.keras</code> today, because it has a nice abstraction and makes it easier to understand the fundamentals of Deep-Learning the first time you encounter them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe3fe6",
   "metadata": {},
   "source": [
    "üè≠ This will also give you valuable experience with the industry standard, and you will use <code>TensorFlow.keras</code> later in the course if you are on **EDSML**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a505d",
   "metadata": {},
   "source": [
    "ü§ñ For your **Deep-Learning Module**, you will work with <code>PyTorch</code>: this will give you experience with it and with all the newest research models out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d89da",
   "metadata": {},
   "source": [
    "‚ú® For your **Independent Research Project** and your future career, you should feel free to use either framework, or indeed any newer and better library out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01422a2",
   "metadata": {},
   "source": [
    "## What is TensorFlow.Keras?\n",
    "\n",
    "TensorFlow is the **backend**, i.e. the compute module, for deep-learning written by Google. Keras, written by Fran√ßois Chollet (Google), is the high-level API sitting on top of TensorFlow and making writing neural networks easy! \n",
    "\n",
    "<img src=\"figures/keras_and_tf.png\" style=\"width:1300px\">\n",
    "<a href=\"https://drek4537l1klr.cloudfront.net/chollet2/v-3/Figures/keras_and_tf.png\">Chollet, J.F.</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad58ea",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è <code>TensorFlow</code> and <code>Keras</code> used to be separate libraries before 2019: when googling for help, make sure to search solutions for **tf.keras**, NOT simply keras! The two framework are very similar, but not fully compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c3a61",
   "metadata": {},
   "source": [
    "## Importing <code>TensorFlow.keras</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import it!\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e66f01",
   "metadata": {},
   "source": [
    "When using `tf.keras` (and more generally whenever building deep-learning models) there are three steps to follow:\n",
    "\n",
    "* 1Ô∏è‚É£ **Define the model architecture** of your model; this will be initiated with random weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634678c0",
   "metadata": {},
   "source": [
    "* 2Ô∏è‚É£ **Define the methods** used to evaluate and train your model: this include the cost function you want to use, learning rates (remember the class on gradient descent?), etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e738445",
   "metadata": {},
   "source": [
    "* 3Ô∏è‚É£ **Fit the model** to your feature vector `X` in order to obtain the best estimates for your weights $\\theta$ in order to predict your $\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecfdd97",
   "metadata": {},
   "source": [
    "# Building A Feedforward Neural Network\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_panda.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>A 35 mm photography of a panda construction worker with a yellow hard hat putting together a complex array of electrical components,  digital art</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b800a1",
   "metadata": {},
   "source": [
    "## Defining model architecture with the <code>tf.keras</code> <code>sequential API</code>\n",
    "\n",
    "The `Sequential` API consists at adding each layer one-by-one to the network. There is also a `Functional` API, where layers are modelled as functions, and the output of each layer is the input of the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b56b49",
   "metadata": {},
   "source": [
    "Let's re-create our very simple neural network model from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a3219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First layer : 3 neurons and ReLU as activation function\n",
    "model.add(layers.Dense(3, activation='relu',input_dim=4)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfafb4",
   "metadata": {},
   "source": [
    "‚û°Ô∏è The `input_dim` captures the shape of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76481be",
   "metadata": {},
   "source": [
    "‚û°Ô∏è The `Dense` class referes to a **fully connected** layer of neurons (in this case, 3 neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c9868",
   "metadata": {},
   "source": [
    "We also need to add the output layer: because this is a binary classifier, we only need one output with the `sigmoid` activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b919b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d350e",
   "metadata": {},
   "source": [
    "That is it, our model is built! Now we can obtain useful information about the architecture of our model by calling the `.summary()` function on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb7618",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355164ab",
   "metadata": {},
   "source": [
    "As you can see, this gives you the type of each layer (both are dense layers), their shape (3, 1) and the number of trainable parameters in each layer and in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57d4f4",
   "metadata": {},
   "source": [
    "## Some Ground Rules for building Neural Networks\n",
    "\n",
    "The problem of course is that you can build very complex architectures. For instance, we can do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429f31d",
   "metadata": {},
   "source": [
    "![architecture](figures/neuralnet_0.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42cbf9",
   "metadata": {},
   "source": [
    "### Many architectures are possible, but there are a few ground rules:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed48b7b",
   "metadata": {},
   "source": [
    "1. <span style=\"color:darkgreen\">**The FIRST LAYER**</span> needs the size of your input (we have seen this above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a70d7e",
   "metadata": {},
   "source": [
    "2. <span style=\"color:brown\">**The LAST LAYER**</span> is dictated by your task: \n",
    "    * A regression task requires a layer with **1 neuron** and a **linear** activation function  \n",
    "    * Binary classification requires a layer with **1 neuron** and a **sigmoid** activation function\n",
    "    * Multiclass classification requires a layer with as many neuron as there are classes, and a **softmax** activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5888f",
   "metadata": {},
   "source": [
    "# A full example using the MNIST digits dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676edcf5",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "<img src=\"figures/mnist.png\" style=\"padding:10px;width:800px;\" align=\"left\"/>\n",
    "\n",
    "<span style=\"color:teal\">**Today's dataset:** </span> Today we will use a classic data, the \"Hello World\" of Deep-Learning: <a href=\"https://paperswithcode.com/dataset/mnist\"> The MNIST Hand Written Digits dataset</a>. It was introduced by **Yann LeCun**, and contains over 60000 training images and 10000 test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "first_image = X_train[0]\n",
    "plt.imshow(first_image, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d04faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868238a8",
   "metadata": {},
   "source": [
    "## Reshape array and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29a375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = X_train.reshape(X_train.shape[0], 28*28)/255\n",
    "X_test_norm = X_test.reshape(X_test.shape[0], 28*28)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa3ff8",
   "metadata": {},
   "source": [
    "## Transform `y's` to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43225bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_oh = to_categorical(y_train, 10)\n",
    "y_test_oh = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7e8a1",
   "metadata": {},
   "source": [
    "## Building ou model\n",
    "\n",
    "Let's build our model! Our model will contain the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22e3be1",
   "metadata": {},
   "source": [
    "0Ô∏è‚É£ an input layer of `28*28` (784) neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1bd1f",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ a first hidden layer of **100 fully-connected neurons** and a **ReLu activation** function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14aa43",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£+3Ô∏è‚É£ Two more hidden layers (**30 & 10 fully-connected neurons**) with **ReLu activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921abffd",
   "metadata": {},
   "source": [
    "4Ô∏è‚É£ a final layer appropriate for our multiclass classification task (10 tasks => **10 neurons** with `softmax` activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(100, activation='relu', input_dim=784))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03328f0",
   "metadata": {},
   "source": [
    "# Compiling and Training our Neural Network\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_train.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>A neural network being trained, digital art</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d50165",
   "metadata": {},
   "source": [
    "## Compiling your model\n",
    "\n",
    "Deep-Learning models need to be `compiled` with `TensorFlow` before we can `fit` them. When we do this, `TensorFlow` builds an optimized calculation graph for our deep-learning model. The minimum parameters we need to specify are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058bf9fa",
   "metadata": {},
   "source": [
    "üõµ What <span style=\"color:red\">**Optimizer**</span> we want to use to solve our loss function (i.e. a flavour of gradient descent or SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aecb8c",
   "metadata": {},
   "source": [
    "üå°Ô∏è What <span style=\"color:Blue\">**loss function**</span> we will use:  common choices include **MAE** and **MSE** (regression), **binary crossentropy** (binary classification), or **categorical crossentropy** (multiclass classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a511e",
   "metadata": {},
   "source": [
    "üìê What <span style=\"color:teal\">**metric**</span> we will use to evaluate our model during training (using the validation set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a37a6b2",
   "metadata": {},
   "source": [
    "## Optimizer (solvers) for Deep-Learning\n",
    "\n",
    "<img src=\"figures/solvers.gif\" style=\"width:1000px\">\n",
    "<a href=\"https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c\">Jiang,L., 2020</a>: link contains really cool animations of Gradient Descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ec38fa",
   "metadata": {},
   "source": [
    "‚õ∞Ô∏è Many flavours of **gradient descent** for deep-learning: *Momentum*, *AdaGrad*, *RMSProp* and *Adam*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac4095f",
   "metadata": {},
   "source": [
    "üö† For all intent and purposes, ***Adam*** (**Ada**ptive **M**oment Estimation) combines the best of all of the previous generations of solvers and is the <span style=\"color:blue\">GoTo</span> solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10493b01",
   "metadata": {},
   "source": [
    "ü´∂ Much more on this in the **Deep-Learning Module** and **Opitimization Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e23d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics = 'Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e34473",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "Once the model is compiled, we can `fit()` it in a manner similar to `scikit-learn`. The parameters we need here are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e7fff",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ Specify the features (`X_train_norm`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6865e2d6",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ Specify the labels  (`y_train_oh`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd6f22",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ Specify what portion (if any) of the training set should be used for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129eee69",
   "metadata": {},
   "source": [
    "4Ô∏è‚É£ Specify the batch size, i.e. how many images should the network try to fit at once. Typical batch sizes are 8, 16, 32, and sometimes 64. Larger batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438a335",
   "metadata": {},
   "source": [
    "5Ô∏è‚É£ Specify how often the full dataset needs to be used in training, i.e. how many  `epochs` to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7486f7",
   "metadata": {},
   "source": [
    "## How are the weights and bias of the model adjusted?\n",
    "<br>\n",
    "<img src=\"figures/backpropagation.gif\" syle=\"width:1600px\">\n",
    "<a href=\"https://machinelearningknowledge.ai/\">MLK, date unknown</a>\n",
    "\n",
    "* Uses derivative of gradients to attribute error for each weights of the network\n",
    "* Computationally efficient as the partial derivatives are computed only once per epoch (during forward pass)\n",
    "* **You will see details** during the **Deep-Learning module**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca8132",
   "metadata": {},
   "source": [
    "#### Let's fit and train the model!\n",
    "\n",
    "* This will take a little while (so we limit oursleves to 8 epochs here).\n",
    "* We also save the training data into an object as we will want to inspect this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf11b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = model.fit(X_train_norm, \n",
    "                       y_train_oh, \n",
    "                       batch_size=256, \n",
    "                       epochs=8, \n",
    "                       validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730496dd",
   "metadata": {},
   "source": [
    "##  Evaluating our model\n",
    "\n",
    "We can test how well our model works by using the `.evaluate` method on the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2199272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.evaluate(X_test_norm, y_test_oh) # N.B. The X_test needs to be prepared as the X_train but you have already scaled it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eab2a",
   "metadata": {},
   "source": [
    "## Using the training curve\n",
    "\n",
    "The training curve is used a lot in **Deep-Learning** to evaluate models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our training data contains a history of training. We can plot it!\n",
    "history = training_data.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6787196",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ad9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(20, 12))\n",
    "ax.plot(history['loss'], label='Training loss');ax.plot(history['val_loss'], label='Validation loss')\n",
    "ax.set_xlabel('nb Epochs', size=16); ax.set_ylabel('Loss', size=16); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717a4d3",
   "metadata": {},
   "source": [
    "##  Specialized layers and conveniences in `TensorFlow`\n",
    "\n",
    "üè∫ `TensorFlow` comes loaded with a lot of convenient functions and classes to process data, build, and train neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5bd08",
   "metadata": {},
   "source": [
    "üåü In the coming weeks, you will learn about some of those in `PyTorch`. Keep in mind that for most (if not all) of them there is a `TensorFlow` equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72a72c",
   "metadata": {},
   "source": [
    "üåº These include specialized layers such as `convolution`, `dropout`, and `maxpool` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f3d40",
   "metadata": {},
   "source": [
    "üê±‚Äçüíª But also facilities to load images, automatically perform data augmentation, and much more. Read the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras\">tensorflow.keras documentation</a> for more info!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1603ff32",
   "metadata": {},
   "source": [
    "üçÄ As an example, we can redo our exerice but rather than manually prepare our digits, we can add a `Flatten` and a `Normalize` layer in our architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f620d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(28,28)))\n",
    "model.add(layers.Rescaling(scale=1./255.))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(30, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda3596c",
   "metadata": {},
   "source": [
    "### Compile our new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics = 'Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09c6dd",
   "metadata": {},
   "source": [
    "### Will it run???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_data = model.fit(X_train, \n",
    "                       y_train_oh, \n",
    "                       batch_size=256, \n",
    "                       epochs=2, \n",
    "                       validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5fddb8",
   "metadata": {},
   "source": [
    "# Suggested Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f22b8b",
   "metadata": {},
   "source": [
    "## üì∫ Videos \n",
    "#### Short videos from my Undegraduate Machine Learning Classes:\n",
    "* üìº <a href=\"https://youtu.be/-ohZINc7OCY?list=PLZzjCZ3QdgQCcRIwQdd-_cJNAUgiEBB_n\">Introduction to Neural Networks</a>\n",
    "* üìº <a href=\"https://youtu.be/A1-HocOPXms?list=PLZzjCZ3QdgQCcRIwQdd-_cJNAUgiEBB_n\">Convolutional Neural Networks</a>\n",
    "\n",
    "#### Others\n",
    "* üìº <a href=\"https://youtu.be/aircAruvnKk\">But what is a neural network? | Chapter 1, Deep learning</a>, by 3Blue1Brown\n",
    "* üìº <a href=\"https://youtu.be/04L4ZHiJbjs\">TensorFlow & Keras Tutorial 2022 | Deep Learning With TensorFlow & Keras</a>, by Simplilearn (> 7 hours of video, full course!)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7612eea",
   "metadata": {},
   "source": [
    "## üìö Further Reading \n",
    "* üìñ <a href=\"https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9\">Machine Learning for Beginners: An Introduction to Neural Networks</a> by Victor Zhou, 2019\n",
    "* üìñ <a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">\n",
    "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition</a> Aur√©lien G√©ron, 2019"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
