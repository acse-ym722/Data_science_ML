{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ef378c",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:maroon\">Model Tuning</h1>\n",
    "    <img src=\"figures/04-model_tuning.jpeg\" style=\"width:1300px;\">\n",
    "    <h3><span style=\"color: #045F5F\">Data Science & Machine Learning for Planet Earth Lecture Series</span></h3><h6><i> by C√©dric M. John <span style=\"size:6pts\">(2022)</span></i></h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2598574",
   "metadata": {},
   "source": [
    "## Plan for today's Lecture üóì "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db731f9e",
   "metadata": {},
   "source": [
    "* Bias/Variance tradeoff\n",
    "* Learning Curve\n",
    "* Model Selection\n",
    "* Regularizing models\n",
    "* Model Tuning\n",
    "* Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2583ee8c",
   "metadata": {},
   "source": [
    "## Intended learning outcomes üë©‚Äçüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ceef7",
   "metadata": {},
   "source": [
    "* Choose a fit for purpose model\n",
    "* Regularize to avoid overfitting\n",
    "* Use SVR and SVC for respectively regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba1885",
   "metadata": {},
   "source": [
    "# \"No Free Lunch\" Theorem\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_freelunch.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>Photo of a beautifully colored greek lunch with olives, mousaka and various other elements set on a white and blue checkered table cloth, sunny bright lighting</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec1120",
   "metadata": {},
   "source": [
    "# Data used today\n",
    "We are back to the <a href=\"https://www.kaggle.com/aungpyaeap/fish-market\">Kaggle fish market dataset</a>.\n",
    "<img src=\"figures/fish_dataset.png\" style=\"width:1300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some packages we will use later to plot...\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('data/fish_no_pikes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d67ae",
   "metadata": {},
   "source": [
    "#  Bias vs Variance \n",
    "<img src=\"figures/variance_tradeoff_bullseye.png\" style=\"width:900px;\">\n",
    "<a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\">Scott-Fortmann, 2012</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aabd49",
   "metadata": {},
   "source": [
    "# The Bias / Variance tradeoff\n",
    "For a model to generalize there will be a tradeoff between **bias** and **variance**.\n",
    "<img src=\"figures/over_under_bias_variance.png\" style=\"width:1300px\">\n",
    "<a href=\"https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\">Singh, 2018</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5ce7f",
   "metadata": {},
   "source": [
    "* **Bias (Underfitting)**: The inability for an algorithm to learn the patterns within a dataset.\n",
    "* **Variance (Overfitting)**: The algorithm generates an overly complex relationship when modelling patterns within a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189d54e",
   "metadata": {},
   "source": [
    "## üêü Testing with our fish dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34253e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import polynomial_regression\n",
    "polynomial_regression(data=df,degrees=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb2300",
   "metadata": {},
   "source": [
    "## No Free Lunch Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a95c2",
   "metadata": {},
   "source": [
    "Some models **oversimplify**, while others **overcomplicate** a relationship between features and target.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0c0fc",
   "metadata": {},
   "source": [
    "It's up to us data scientists to make **assumptions** about the data and evaluate reasonable models accordingly.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b937b9d",
   "metadata": {},
   "source": [
    "**There is no one size fits all model**, this is known as the **No Free Lunch Theorem**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbcb32",
   "metadata": {},
   "source": [
    "## The Learning Curves\n",
    "We can use learning curves to diagnose three aspects of model behaviour on the dataset:\n",
    "* Underfitting\n",
    "* Overfitting\n",
    "* Whether the model has sufficient data to learn the patterns of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a80e3",
   "metadata": {},
   "source": [
    "### Concept\n",
    "Increasing the size of the training set can affect the training and validation scores.\n",
    "![lc](figures/learning_curves_transposed.png)\n",
    "<p><a href=\"https://www.dataquest.io/blog/learning-curves-machine-learning/\">Olteanu, 2018</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcfa9d4",
   "metadata": {},
   "source": [
    "### Reading  the curves\n",
    "As the training size increases:\n",
    "* The training score will decrease\n",
    "* The testing score will increase\n",
    "* The curves typically (but not always!) demonstrate convergence\n",
    "![lc1](figures/learning_curve_1.png)\n",
    "<p><a href=\"https://www.dataquest.io/blog/learning-curves-machine-learning/\">Olteanu, 2018</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_train, lc_test = train_test_split(df, train_size=.8, random_state=3)\n",
    "val_score = []\n",
    "train_score = []\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "for nb_samples in range(1,100):\n",
    "    data = lc_train.iloc[:nb_samples]\n",
    "    poly_tr = PolynomialFeatures(degree=2).fit(data.drop(columns=['Weight', 'Species']))\n",
    "    poly = poly_tr.transform(data.drop(columns=['Weight', 'Species']))\n",
    "    lin_model = LinearRegression().fit(poly, data.Weight)\n",
    "    y_pred = lin_model.predict(poly)\n",
    "    y_test_pred = lin_model.predict(poly_tr.transform(lc_test.drop(columns=['Weight', 'Species'])))\n",
    "\n",
    "    train_score.append(np.sqrt(mean_squared_error(data.Weight, y_pred)))\n",
    "    val_score.append(np.sqrt(mean_squared_error(lc_test.Weight, y_test_pred)))\n",
    "                     \n",
    "ax.plot(train_score, label='Training RMSE')\n",
    "ax.plot(val_score, label='Testing RMSE')\n",
    "ax.set_xlabel('Number of training samples', size = 14)\n",
    "ax.set_ylabel('RMSE', size = 14)\n",
    "ax.legend();\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f647e6f",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"The-Bias-Variance-Tradeoff\">The Bias-Variance Tradeoff</h3><p>One of the most important concepts in Data science!</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad578c",
   "metadata": {},
   "source": [
    " Measuring the error on an unseen **Test set**:<br>\n",
    "<img src=\"figures/biasvariancetradeoff.png\" style=\"width:1300px;\">\n",
    "$$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "üìö <a href=\"https://hastie.su.domains/ElemStatLearn/\">Hastie et al, 2009 (Elements of Statistical Learning)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621bfab",
   "metadata": {},
   "source": [
    "Best model complexity is the one reducing the **Total Error** on a unseen dataset\n",
    "<img src=\"figures/model_complexity_error_training_test.jpg\" style=\"width:1300px;\">\n",
    "üìö <a href=\"https://hastie.su.domains/ElemStatLearn/\">Hastie et al, 2009 (Elements of Statistical Learning)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2181d7",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_selection.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>A large wooden cheeseboard with five types of cheese, various small decorative flowers and tomatoes, gurken, pickled onions, and a cheese knife with a</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec9f89",
   "metadata": {},
   "source": [
    "# What data to use to select the right model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc2c2d5",
   "metadata": {},
   "source": [
    "* Do NOT use the **test set**: would lead to overfitting\n",
    "* Instead use a *Validation Set*\n",
    "<p><img alt=\"image.png\" src=\"data:image/png;base64,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\"/></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd8c38",
   "metadata": {},
   "source": [
    "\n",
    "* Even better: <strong>cross-validate</strong> instead of a using single holdout val set cross validate with the train set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309c9f96",
   "metadata": {},
   "source": [
    "### Model complexity vs dataset size (rule of thumb üëç)\n",
    "\n",
    "* More than 100,000 datapoints: Parametric models (SGD, Neural Nets)\n",
    "* Less than 100,000 datapoints: Non-parametric models (KNN, SVM, Decision Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10224cee",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/ml_cheat_sheet.png\" style=\"width:1500px;\">\n",
    "<a href=\"https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\"><code>sklearn</code> algorithm cheat sheet</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad896fd",
   "metadata": {},
   "source": [
    "### What to do if the model overfits?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd194f0",
   "metadata": {},
   "source": [
    "\n",
    "<blockquote><p><em>Simplify</em> your model <em>relatively</em> to your data</p>\n",
    "</blockquote>\n",
    "<ul>\n",
    "<li>Choose a simpler model</li>\n",
    "<li>Get more observations</li>\n",
    "<li>Feature selection (manual or <a href=\"https://scikit-learn.org/stable/modules/feature_selection.html\">automated</a>)</li>\n",
    "<li>Dimensionality reduction (Unsupervised Learning)</li>\n",
    "<li>Early stopping (Deep Learning)</li>\n",
    "<li><strong>Regularization</strong> of your Loss function</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2025c5c4",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e0f96",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"./figures/feature_selection.png\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d3a52",
   "metadata": {},
   "source": [
    "Feature selection is the process of eliminating non-informative features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacabd78",
   "metadata": {},
   "source": [
    "### The curse of dimensionality\n",
    "Not observing enough data to support a meaningful relationship.\n",
    "<img align=\"center\" src=\"./figures/curse_of_dimensionality.png\" width=\"800px\"/>\n",
    "<a href=\"https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/\">Source</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2f2d5",
   "metadata": {},
   "source": [
    "\n",
    "<img align=\"center\" src=\"figures/curse_of_dimensionality_boxes.png\" width=\"1200px\"/>\n",
    "As the number of features or dimensions grows, the amount of data we need to generalise accurately grows **exponentially** e.g $5^1$, $5^2$, $5^3$, $5^n$\n",
    "<a href=\"https://www.freecodecamp.org/news/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335/\">Source</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856fbd3",
   "metadata": {},
   "source": [
    "## Feature correlation\n",
    "One selection technique is to remove one (or more) of  features that are highly correlated to each other.\n",
    "* High correlation = redundant information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ae0a1",
   "metadata": {},
   "source": [
    "### üñ• Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b57b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = X_train.copy()\n",
    "corr_data['target'] = y_train\n",
    "corr = corr_data.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(1,1, figsize=(9,8))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns,\n",
    "        cmap= \"seismic\",ax=ax, annot=corr);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d962cc1",
   "metadata": {},
   "source": [
    "## Model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c48fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "scores = cross_val_score(LinearRegression(), X_train, y_train, cv=10)\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89e6d5",
   "metadata": {},
   "source": [
    "## Model with only best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "scores = cross_val_score(LinearRegression(), X_train[['Length3', 'Width', 'Height']], y_train, cv=10)\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02682c9a",
   "metadata": {},
   "source": [
    "Colinearities are not too high in this dataset, but if we had seen high colinearities we could have done the following:\n",
    "* Drop one of the two variables that are strongly co-linear\n",
    "* Drop more than one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492e3988",
   "metadata": {},
   "source": [
    "# Feature Permutation \n",
    "Feature permutation is a second feature selection algorithm that evaluates the importance of each feature in predicting the target.\n",
    "* Trains and records the test score of a base model containing all features \n",
    "* Randomly shuffles (permutation) <span style=\"color:blue\">**one**</span> feature within the test set \n",
    "* Records new score on shuffled test set \n",
    "* Compares the new score to the original score \n",
    "* Repeat for each feature \n",
    "üëâ <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html\">Sklearn's <code>permutation_importance</code> documentation</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e9da6",
   "metadata": {},
   "source": [
    "üëâ If the score drops when a feature is shuffled, it is considered important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e81344",
   "metadata": {},
   "source": [
    "### üíª Feature permutation in Sklearn\n",
    "\n",
    "This time, let's see if turning this problem into a classification problem works better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder().fit(data.Species[X_train.index])\n",
    "\n",
    "y_train_cat = encoder.transform(data.Species[X_train.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2abee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "model = LogisticRegression(max_iter=3000)\n",
    "\n",
    "model.fit(X_train, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0979ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_score = permutation_importance(model, X_train, y_train_cat, n_repeats=10) # Perform Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame(np.vstack((X_train.columns,\n",
    "                                        permutation_score.importances_mean)).T) # Unstack results\n",
    "importance_df.columns=['feature','score decrease']\n",
    "\n",
    "importance_df.sort_values(by=\"score decrease\", ascending = False) # Order by importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8784d7",
   "metadata": {},
   "source": [
    "## Model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "base_model = LogisticRegression(max_iter=3000)\n",
    "\n",
    "scores = cross_val_score(base_model, X_train, y_train_cat, cv=10)\n",
    "\n",
    "scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6227791",
   "metadata": {},
   "source": [
    "## Model with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aeb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_small = X_train[[\"Length3\",\"Length2\", \"Height\"]] # Keep strong features\n",
    "\n",
    "final_model = LogisticRegression(max_iter=5000)\n",
    "\n",
    "scores = cross_val_score(final_model, X_small, y_train_cat, cv=10)\n",
    "\n",
    "scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c1e50",
   "metadata": {},
   "source": [
    "### Reducing Complexity\n",
    "The most simple solution is normally the best solutionüî™\n",
    "Reducing the number of features makes the model:\n",
    "* More interpretable \n",
    "* Faster to train \n",
    "* Easier to implement and maintain in production "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeebecc4",
   "metadata": {},
   "source": [
    "# Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bd58f",
   "metadata": {},
   "source": [
    "Regularization means adding a **penalty term** to the Loss that **increases** with $\\beta$\n",
    "$$\\text{Regularized Loss} = Loss(X,y, \\beta) + Penalty(\\beta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec634c5",
   "metadata": {},
   "source": [
    "üëâ Penalizes large values for $\\beta_i$<br>\n",
    "üëâ Forces model to shrink certain coefficients or even select less features<br>\n",
    "üëâ Prevents overfitting <br>\n",
    "$$\\hat{y} =  \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\beta_3 X_1^3 + ... $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e176d75",
   "metadata": {},
   "source": [
    "Two famous Regularization penalties:<br><br>\n",
    "**Lasso** (L1)<br>\n",
    "$$L1 = Loss  + \\alpha \\sum_{i=1}^n |\\beta_i|$$ <br>\n",
    "**Ridge** (L2)<br>\n",
    "$$L2 = Loss + \\alpha \\sum_{i=1}^n \\beta_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d1dc3",
   "metadata": {},
   "source": [
    "Introduces the new hyper-parameter $\\alpha$:\n",
    "* Dictates **how much** the model is **regularized**\n",
    "* Large $\\alpha$ force **model complexity and variance to decrease**, but **bias increases**\n",
    "* Notice $\\sum$ starts from $i=1$, i.e. intercept coefficient is not penalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce11f0",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è   Always **scale** your feature before regularization to penalize each $\\beta_i$ fairly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325153ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "\n",
    "def test_reg_coefs(data=lc_train):\n",
    "    X = data[['Length1','Length2','Length3', 'Height']]\n",
    "\n",
    "    poly_tr = PolynomialFeatures(degree=2).fit(X)\n",
    "    X = pd.DataFrame(poly_tr.transform(X))\n",
    "    X = pd.DataFrame(StandardScaler().fit_transform(X),columns=X.columns)\n",
    "\n",
    "    # The target is the weight of a fish\n",
    "    Y = data[['Weight']] \n",
    "\n",
    "    linreg = LinearRegression().fit(X, Y)\n",
    "    ridge = Ridge(alpha=1.5, max_iter=2000).fit(X, Y)\n",
    "    lasso = Lasso(alpha=1.5, max_iter=2000).fit(X, Y)\n",
    "\n",
    "    coefs = pd.DataFrame({\n",
    "        \"coef_linreg\": pd.Series(linreg.coef_[0], index = X.columns),\n",
    "        \"coef_ridge\": pd.Series(ridge.coef_[0], index = X.columns),\n",
    "        \"coef_lasso\": pd.Series(lasso.coef_, index= X.columns)})\\\n",
    "\n",
    "    return coefs\\\n",
    "        .applymap(lambda x: int(x))\\\n",
    "        .style.applymap(lambda x: 'color: red' if x == 0 else 'color: black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reg_coefs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f938a64f",
   "metadata": {},
   "source": [
    "# Model Tuning: Finding the best Hyperparameters\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_tuning.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>A jazz band playing a happy tune on their saxophone in the lively streets of New Orleans, vivid colors</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6284ce09",
   "metadata": {},
   "source": [
    "### The grid search method\n",
    "Explores different hyperparam value combinations to find those optimizing performance\n",
    "<img src=\"figures/grid_search.png\" style=\"width:600px;\">\n",
    "<a href=\"https://medium.com/@jackstalfort/hyperparameter-tuning-using-grid-search-and-random-search-f8750a464b35\">Stalfort, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25898818",
   "metadata": {},
   "source": [
    "* Also applied using a *validation set* (never use test set for model tuning!)\n",
    "* Select which grid of values of hyper-parameters to try out\n",
    "* For each combinations of values, measure your performance on the *validation set*\n",
    "* Select hyperparams that produce the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816e79a",
   "metadata": {},
   "source": [
    "**üî• Grid Search CV**<br>\n",
    "<img src=\"figures/full_workflow_validation.png\" style=\"width:1200px;\">\n",
    "<a href=\"https://stats.stackexchange.com/questions/424477/how-to-make-train-test-split-with-given-class-weights\">StackExchange, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b99db",
   "metadata": {},
   "source": [
    "### Sklearn  <code>GridSearchCV</code> üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(columns=['Species', 'Weight'])\n",
    "y = data[['Weight']]\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "# Instanciate model\n",
    "model = Ridge()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'alpha': [0.01, 0.1, 1], \n",
    "        'solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg']}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = GridSearchCV(model, grid, \n",
    "                           scoring = 'r2',\n",
    "                           cv = 5,\n",
    "                           n_jobs=-1 # paralellize computation\n",
    "                          ) \n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1751a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Params\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6cf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator\n",
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4c23b",
   "metadata": {},
   "source": [
    "\n",
    "<p>üëé Limitations of Grid Search:</p>\n",
    "<ul>\n",
    "<li>Computationally costly</li>\n",
    "<li>The optimal hyperparameter value can be missed</li>\n",
    "<li>Can overfit hyperparameters to the training set if too many combinations are tried out for too small a dataset</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc002a",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8accefff",
   "metadata": {},
   "source": [
    "Randomly explore hyperparameter values from:\n",
    "* A hyperparameter space to randomly sample from\n",
    "* The specified number of samples to be tested<br>\n",
    "<img src=\"figures/grid_search2.png\" style=\"width:1200px;\">\n",
    "<a href=\"https://medium.com/@jackstalfort/hyperparameter-tuning-using-grid-search-and-random-search-f8750a464b35\">Stalfort, 2019</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "# Instanciate model\n",
    "model = Ridge()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg'], 'alpha': stats.loguniform(0.01,1)}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = RandomizedSearchCV(model, grid, \n",
    "                            scoring='r2',\n",
    "                            n_iter=100,  # number of draws\n",
    "                            cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(X_train, y_train)\n",
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d45a7",
   "metadata": {},
   "source": [
    "### Using a probability distribution in RandomSearch\n",
    "Can be generated with <a href=\"https://docs.scipy.org/doc/scipy/reference/stats.html\">scipy.stats.distributions</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dist = stats.norm(10, 2) # if you have a best guess (say: 10)\n",
    "\n",
    "dist = stats.randint(1,100) # if you have no idea\n",
    "dist = stats.uniform(1, 100) # same\n",
    "\n",
    "dist = stats.loguniform(0.01, 1) # Coarse grain search\n",
    "\n",
    "r = dist.rvs(size=10000) # Random draws\n",
    "plt.hist(r);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8e3ba",
   "metadata": {},
   "source": [
    "### Limitations of GridSearch and RandomizedSearch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b0eb1",
   "metadata": {},
   "source": [
    "* Both algorithms are not tracking the history of optimization üìú"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a4126",
   "metadata": {},
   "source": [
    "* Choice of next set of parameter is random üé≤ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f3bb9",
   "metadata": {},
   "source": [
    "* Evaluation of the loss function is costly üí∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6ec80",
   "metadata": {},
   "source": [
    "## Bayesian Optimization\n",
    "<p><img src=\"figures/bayes_theorem_visual.jpeg\" style=\"width:600px;\"></p>\n",
    "<a href=\"https://luminousmen.com/post/data-science-bayes-theorem\">Source: Bayes Theorem in Data Science</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e23af",
   "metadata": {},
   "source": [
    "### Principle of Bayesian Optimization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8730a",
   "metadata": {},
   "source": [
    "* Build a surragate function that is quick to evaluate (prior) üé≠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf84242",
   "metadata": {},
   "source": [
    "* Select the next set of hyperparameters based on the surrogate function, evaluate them on the loss function ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4410d1",
   "metadata": {},
   "source": [
    "* Update the surroage function (posterior) üï∞Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c64d83",
   "metadata": {},
   "source": [
    "<img src=\"figures/bayesOpt1.png\" style=\"width:800\">\n",
    "<a href=\"https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\">Koehrsen, 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce20f7",
   "metadata": {},
   "source": [
    "<img src=\"figures/bayesOpt2.png\" style=\"width:800\">\n",
    "<a href=\"https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\">Koehrsen, 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6beaed7",
   "metadata": {},
   "source": [
    "## Bayesian Optimization Implementation\n",
    "We can use BayesSearch <code>skopt</code> do do Bayesian Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaaea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "X = data.drop(columns=['Species', 'Weight'])\n",
    "y = data[['Weight']]\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=.3, random_state=0)\n",
    "\n",
    "#‚ö†Ô∏è Polynomial Transformation first ‚ö†Ô∏è\n",
    "poly_tr = PolynomialFeatures(degree=3).fit(X_train)\n",
    "X_train = poly_tr.transform(X_train)\n",
    "X_test = poly_tr.transform(X_test)\n",
    "    \n",
    "    \n",
    "#‚ö†Ô∏è Data must be centered around its mean before applying PCA ‚ö†Ô∏è\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "opt = BayesSearchCV(\n",
    "    Lasso(max_iter=2000),\n",
    "    {\n",
    "        'alpha': Real(0.1, 10, prior='log-uniform'),\n",
    "        'max_iter': [3000, 5000]\n",
    "    },\n",
    "    n_iter=32,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "print(\"val. score: %s\" % opt.best_score_)\n",
    "print(\"test score: %s\" % opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe87010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_objective, plot_histogram\n",
    "\n",
    "_ = plot_objective(opt.optimizer_results_[0],\n",
    "                   dimensions=[\"alpha\", \"max_iter\"],\n",
    "                   n_minimum_search=int(1e8), size=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b5713",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_svm.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>Two Doric columns supporting a temple in a soft green light, digital art</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80a0ee",
   "metadata": {},
   "source": [
    "### What is the optimal decision boundary for this classification?\n",
    "<img src=\"figures/SVM_planes.png\" style=\"width:1200px\">\n",
    "<a href=\"https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c\">Ippoloto, 2019</a>\n",
    "<p>Infinite number of potential decision boundaries that separate the classes (\"hyperplanes\")</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a533ce4",
   "metadata": {},
   "source": [
    "<img src=\"figures/SVM_margin.png\" style=\"width:1200px;\">\n",
    "<a href=\"https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c\">Ippoloto, 2019</a>\n",
    "\n",
    "* The hyperplane that generalizes best to unseen data is the one that is furthest from all the points (maximizes the **margin**)\n",
    "* The points on the margin boundary are called **support vectors**\n",
    "* Finding them is a convex optimization problem (one single best solution)\n",
    "* **Maximum Margin Classifier** algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b563ea",
   "metadata": {},
   "source": [
    "* Max Margin is super sensitive to outliers\n",
    "* It **overfits** to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a643a0",
   "metadata": {},
   "source": [
    "For **generalization** purpose, we may want to allow some points to be **inside** the margin, or even **on the other side** of the decision boundary:<br>\n",
    "<img src=\"figures/1_M_3iYollNTlz0PVn5udCBQ.png\" style=\"width:900px;\">\n",
    "<a href=\"https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe\">Mishra, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc26585",
   "metadata": {},
   "source": [
    "### Soft margin classifier\n",
    "Allows a few points to be misclassified but with a **penalty ($\\xi$)** for how \"far\" they lie on the wrong side of the margin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a31a55",
   "metadata": {},
   "source": [
    "The **Hinge Loss** is the penalty applied to each point on the wrong side<br>\n",
    "* The deeper a point lies within the margin, the higher the loss\n",
    "* The penalty is linear, like MAE <br>\n",
    "<img src=\"figures/1_M_3iYollNTlz0PVn5udCBQ.png\" style=\"width:800px;\">\n",
    "<a href=\"https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe\">Mishra, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24249b5",
   "metadata": {},
   "source": [
    "<img src=\"figures/hinge_loss_.png\" style=\"width:1300px;\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5527ed",
   "metadata": {},
   "source": [
    "### Regulariation hyperparameter <code>C</code>\n",
    "Stength of the penalty applied on points being on the wrong side of the margin\n",
    "* The higher <code>C</code>, the stricter the margin\n",
    "* A \"maximum margin classifier\" has <code>C</code> = $+ \\infty$\n",
    "* The smaller <code>C</code>, the softer the margin, the more it is ***regularized***\n",
    "* C similar to $1/ \\alpha$ in Ridge \n",
    "<img src=\"figures/svm_regularization.png\" style=\"width:1200px;\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3eeb45",
   "metadata": {},
   "source": [
    "\n",
    "<p>üíª sklearn implementation</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear', C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4643d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent but with SGD solver\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "svc_bis = SGDClassifier(loss='hinge', penalty='l2', alpha=1/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049facf",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning: All support vector models requires **scaling**\n",
    "<img src=\"figures/svm_scaling.png\" style=\"width:1800px;\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c60d1",
   "metadata": {},
   "source": [
    "# SVM 'kernels'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84b600",
   "metadata": {},
   "source": [
    "<img src=\"figures/SVM_nonlin.png\" style=\"width:800px\">\n",
    "\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c108e4c",
   "metadata": {},
   "source": [
    "## We can create more features to separate this data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45326ba",
   "metadata": {},
   "source": [
    "<img src=\"figures/SVM_kernel1.png\" style=\"width:1300px\">\n",
    "\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435feef",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Creates new features - curse of dimensionality!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548f58b",
   "metadata": {},
   "source": [
    "üåΩ Instead, we can use a mathematical <strong style=\"color:teal\">'kernel'</strong> to simulate new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8677c",
   "metadata": {},
   "source": [
    "# The kernel-trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d65d0",
   "metadata": {},
   "source": [
    "üìè Measure a distance pair-wise between each sample and use this to simulate creating new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232fc979",
   "metadata": {},
   "source": [
    "* ***Linear kernel function*** for linear datasets (the best for high-dimensional datasets - speed!)<br>$F(x, xj) = sum( x.xj)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa98d95",
   "metadata": {},
   "source": [
    "* ***Polynomial kernel functions*** (example of previous slide with $X^3$)<br> $F(x, xj) = (x.xj+1)^d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f16bf",
   "metadata": {},
   "source": [
    "* ***Gaussian Radial Basis function kernel (RBF)*** (one of the favourite kernels for non-linear datasets)<br> $F(x, xj) = \\exp(-\\gamma * ||x - xj||^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e32dc",
   "metadata": {},
   "source": [
    "* ***Sigmoid kernel function*** <br> $F(x, xj) = tanh(Œ±xay + c)$<br><br><a href=\"https://dataaspirant.com/svm-kernels/\">Good reference on SVMs with kernels</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ca019",
   "metadata": {},
   "source": [
    "## SVM-Regressors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1fa81",
   "metadata": {},
   "source": [
    "Reverse the objective:\n",
    "* **Classification**: fit the largest possible *street* **between** two classes\n",
    "* **Regression**: fit as many points as possible **within** the *street*\n",
    "* Width of the street controlled by an additional hyperparam $\\epsilon$\n",
    "<img src=\"figures/svm_regressor.png\" style=\"width:1200px\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecbdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "regressor = SVR(epsilon=1, kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830b7f7",
   "metadata": {},
   "source": [
    "# Suggested Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ca430",
   "metadata": {},
   "source": [
    "## üì∫ Videos \n",
    "#### Short videos from my Undegraduate Machine Learning Classes:\n",
    "* üìº <a href=\"https://youtu.be/8mNPHGmXS5Q?list=PLZzjCZ3QdgQCcRIwQdd-_cJNAUgiEBB_n\">Support Vector Machines</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ec821",
   "metadata": {},
   "source": [
    "## üìö Further Reading \n",
    "* üìñ <a href=\"https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\">Support Vector Machine ‚Äî Introduction to Machine Learning Algorithms</a> by Rohith Gandhi, 2018\n",
    "* üìñ <a href=\"https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf\">Taking the Human Out of the Loop: A Review of Bayesian Optimization</a> by Shariari et al\n",
    "* üìñ <a href=\"https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\">A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning</a> by Will Koehrsen"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
